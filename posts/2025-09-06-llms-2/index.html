






<!doctype html>
<html
  lang="en-us"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="dark"
  data-auto-appearance="true"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#FFFFFF" />
  
  <title>Large Language Models, pt. 2: Safety and Hygiene &middot; Eric&#39;s Web.site</title>
    <meta name="title" content="Large Language Models, pt. 2: Safety and Hygiene &middot; Eric&#39;s Web.site" />
  
  
  
  
  
  <script
    type="text/javascript"
    src="http://localhost:1313/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js"
    integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="
  ></script>
  
  
  
  
  
  
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="http://localhost:1313/css/main.bundle.min.8363fdb90097634558e4c060565c2caefbddbf21152432d1b5ca1f55adca4e75.css"
    integrity="sha256-g2P9uQCXY0VY5MBgVlwsrvvdvyEVJDLRtcofVa3KTnU="
  />
  
  
  
  
  
  
  
  <meta
    name="description"
    content="
      
        This is part of a multi-part series of essays on LLMs. 
      
    Start from the beginning or view all posts?
Safety #In the discourse on LLMs, &ldquo;safety&rdquo; has been an important and multifaceted topic. The main way that it&rsquo;s talked about it is through the lens of &ldquo;What if someone uses a LLM to get bomb instructions?&rdquo;. I tend to think these fears are overstated; this information has long been available on the internet in various places—the risk does not seem meaningfully different to the risks before LLMs. It&rsquo;s not a new risk. Au contraire, I think one should be suspicious of the tech industry when they talk about safety; it&rsquo;s a flag often waved to argue that corporations and government should have more control over the software, and the that the consumer should have less.
      
    "
  />
  
  
  
  
    <link rel="canonical" href="http://localhost:1313/posts/2025-09-06-llms-2/" />
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/posts/2025-09-06-llms-2/">
  <meta property="og:site_name" content="Eric&#39;s Web.site">
  <meta property="og:title" content="Large Language Models, pt. 2: Safety and Hygiene">
  <meta property="og:description" content="This is part of a multi-part series of essays on LLMs. Start from the beginning or view all posts?
Safety #In the discourse on LLMs, “safety” has been an important and multifaceted topic. The main way that it’s talked about it is through the lens of “What if someone uses a LLM to get bomb instructions?”. I tend to think these fears are overstated; this information has long been available on the internet in various places—the risk does not seem meaningfully different to the risks before LLMs. It’s not a new risk. Au contraire, I think one should be suspicious of the tech industry when they talk about safety; it’s a flag often waved to argue that corporations and government should have more control over the software, and the that the consumer should have less.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-04T13:23:29-05:00">
    <meta property="article:modified_time" content="2025-09-04T13:23:29-05:00">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Large Language Models, pt. 2: Safety and Hygiene">
  <meta name="twitter:description" content="This is part of a multi-part series of essays on LLMs. Start from the beginning or view all posts?
Safety #In the discourse on LLMs, “safety” has been an important and multifaceted topic. The main way that it’s talked about it is through the lens of “What if someone uses a LLM to get bomb instructions?”. I tend to think these fears are overstated; this information has long been available on the internet in various places—the risk does not seem meaningfully different to the risks before LLMs. It’s not a new risk. Au contraire, I think one should be suspicious of the tech industry when they talk about safety; it’s a flag often waved to argue that corporations and government should have more control over the software, and the that the consumer should have less.">

  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "Large Language Models, pt. 2: Safety and Hygiene",
    "headline": "Large Language Models, pt. 2: Safety and Hygiene",
    
    "abstract": "\u003cp\u003eThis is part of a multi-part series of essays on LLMs. \n      \n    \u003ca href=\u0022http:\/\/localhost:1313\/posts\/2025-09-05-llms-1\/\u0022\u003eStart from the beginning\u003c\/a\u003e or \u003ca href=\u0022\/categories\/LLMs\u0022\u003eview all posts\u003c\/a\u003e?\u003c\/p\u003e\n\u003ch1 id=\u0022safety\u0022 class=\u0022relative group\u0022\u003eSafety \u003cspan class=\u0022absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\u0022\u003e\u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022 style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#safety\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\u003c\/span\u003e\u003c\/h1\u003e\u003cp\u003eIn the discourse on LLMs, \u0026ldquo;safety\u0026rdquo; has been an important and multifaceted topic. The main way that it\u0026rsquo;s talked about it is through the lens of \u0026ldquo;What if someone uses a LLM to get bomb instructions?\u0026rdquo;. I tend to think these fears are overstated; this information has long been available on the internet in various places—the risk does not seem meaningfully different to the risks before LLMs. It\u0026rsquo;s not a \u003cem\u003enew\u003c\/em\u003e risk. Au contraire, I think one should be suspicious of the tech industry when they talk about safety; it\u0026rsquo;s a flag often waved to argue that corporations and government should have more control over the software, and the that the consumer should have less.\u003c\/p\u003e",
    "inLanguage": "en-us",
    "url" : "http:\/\/localhost:1313\/posts\/2025-09-06-llms-2\/",
    "author" : {
      "@type": "Person",
      "name": "Eric Miller"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-09-04T13:23:29-05:00",
    "datePublished": "2025-09-04T13:23:29-05:00",
    
    "dateModified": "2025-09-04T13:23:29-05:00",
    
    "keywords": ["llms","opinion","essay","hygiene","safety","hallucination"],
    
    "mainEntityOfPage": "true",
    "wordCount": "1880"
  }
  </script>
    
    <script type="application/ld+json">
    {
   "@context": "https://schema.org",
   "@type": "BreadcrumbList",
   "itemListElement": [
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/",
       "name": "Index",
       "position": 1
     },
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/posts/",
       "name": "Posts",
       "position": 2
     },
     {
       "@type": "ListItem",
       "name": "Large Language Models, Pt. 2 Safety and Hygiene",
       "position": 3
     }
   ]
 }
  </script>

  
  
    <meta name="author" content="Eric Miller" />
  
  
    
      <link href="https://github.com/sosheskaz/" rel="me" />
    
      <link href="https://www.linkedin.com/in/eric-miller/" rel="me" />
    
  
  
  







  
  

  
  
</head>
<body
    class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >Eric&rsquo;s Web.site</a
  >

    </div>
    
    
      <ul class="flex list-none flex-col text-end sm:flex-row">
        
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/posts/"
                  title="Posts"
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >All Posts</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/categories/"
                  title="Categories"
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Categories</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/index.xml"
                  title=""
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >RSS</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/profile/"
                  title="About Me"
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >About</span
                    >
                  </a
                >
              
            </li>
          
          
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex grow flex-col">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
        <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/"
      >Index</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class=" inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/posts/"
      >Posts</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden inline">
    <a
      class="dark:underline-neutral-600 decoration-neutral-300 hover:underline"
      href="http://localhost:1313/posts/2025-09-06-llms-2/"
      >Large Language Models, pt. 2: Safety and Hygiene</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


      
      <h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        Large Language Models, pt. 2: Safety and Hygiene
      </h1>
      
        <div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
          





  
  



  

  
  
    
  

  

  

  
    
  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2025-09-04 13:23:29 -0500 CDT">September 4, 2025</time><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">9 mins</span>
    

    
    
      <span class="ps-2"><span class="flex">
  <span
    class="ms-1 rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400"
  >
    Draft
  </span>
</span>
</span>
    
  </div>

  
  


        </div>
      
      
    </header>
    <section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row">
      
      <div class="min-h-0 min-w-0 max-w-prose grow">
        <p>This is part of a multi-part series of essays on LLMs. 
      
    <a href="http://localhost:1313/posts/2025-09-05-llms-1/">Start from the beginning</a> or <a href="/categories/LLMs">view all posts</a>?</p>
<h1 id="safety" class="relative group">Safety <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#safety" aria-label="Anchor">#</a></span></h1><p>In the discourse on LLMs, &ldquo;safety&rdquo; has been an important and multifaceted topic. The main way that it&rsquo;s talked about it is through the lens of &ldquo;What if someone uses a LLM to get bomb instructions?&rdquo;. I tend to think these fears are overstated; this information has long been available on the internet in various places—the risk does not seem meaningfully different to the risks before LLMs. It&rsquo;s not a <em>new</em> risk. Au contraire, I think one should be suspicious of the tech industry when they talk about safety; it&rsquo;s a flag often waved to argue that corporations and government should have more control over the software, and the that the consumer should have less.</p>
<p>Further, &ldquo;safety&rdquo; of this kind comes with additional problems. LLMs make inferences based on all of the data they&rsquo;re trained on, and the more gaps exist, the more likely it is that important information existed in that gap. The data in those is relevant to the censored data, but it may also be relevant to other data. Most approaches to censoring LLMs come with side effects, like increased hallucination rates and improper refusals. They also don&rsquo;t have a great track record of being bulletproof, especially given time for people to find bypasses. Improving safety in this way, generally makes the tool function worse for safe queries.</p>
<p>The safety angle that I think is far more important, stems how people interact with LLMs, and the novelties that LLMs have. LLMs are able to present themselves like sentient beings. They are able to role-play, they have <a href="https://www.popularmechanics.com/technology/robots/a64423163/turing-test-gpt-45/" target="_blank" rel="noreferrer">officially passed the Turing test</a>, they are able to write better than the median American.</p>
<h2 id="hallucination" class="relative group">Hallucination <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#hallucination" aria-label="Anchor">#</a></span></h2><p>The most immediate safety problem presented by LLMs is &ldquo;hallucination&rdquo;. This is a colloquial term for when a LLM provides factually incorrect information, often completely made-up. This is a problem because LLMs are notoriously bad at saying &ldquo;I don&rsquo;t know&rdquo;. In particular, this mistake has been made <a href="https://www.damiencharlotin.com/hallucinations/" target="_blank" rel="noreferrer">by lawyers many times</a>. When asking an LLM for a legal argument, it <strong>will</strong> come up with one, whether there is a legal basis or not, and it will have citations, whether those cases exist or not, and whether they agree with the legal argument or not.</p>
<p>This gets to a core characteristic of LLMs. They are good at giving opinions, but bad at giving facts. One should not trust facts given to them by an LLM. An LLM may do a good job of accepting a list of relevant facts and summarizing them, or making inferences on them, but facts that the LLM itself produces cannot be trusted to be rooted in reality. While they do better with <em>opinions</em>, where it is helpful to be <em>directionally accurate</em> but not necessarily factual, the error rates are just too high to be used for specifics, when specifics matter.</p>
<p>This factuality gap remains a risk today, although it is getting better. It is necessary for users of LLMs to take responsibility for the veracity of facts used or given by LLMs. They can&rsquo;t be used absent-mindedly in that respect.</p>
<h2 id="language-and-humanity" class="relative group">Language and Humanity <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#language-and-humanity" aria-label="Anchor">#</a></span></h2><p>This brings us to a unique place in history. For most of history, most people have been illiterate. Until very recently, every bit of text ever read was authored by a human, or at least downstream of a human. This is something that we understand at an emotional level. Humans are exceptional animals in many ways, but two important ones are socialization and pattern recognition. These make us see ourselves everywhere; we see a man in the moon, we see people and faces in clouds, we see faces in inanimate objects. This is something we are all hard-wired to do, on an emotional level, and that we always carry with us. Given that, when we read a sentence that can&rsquo;t be distinguished from human, I think something in our lower brains will likely struggle to distinguish it from human, even if intellectually we know better, and this could be dangerous.</p>
<p>This class of technology is something that I worry about, given recent history with social media. We&rsquo;ve seen that social media can have significantly negative effects on mental health, especially in children. Today, we know that it&rsquo;s driven by manipulations of psychology, that gamification and the like are used to keep people active on the platform, even if they don&rsquo;t enjoy it, and that it manipulates the social parts of our brains. If you&rsquo;ve ever had a post go big on social media, you know the feeling of validation as likes/points pour in, and for those who become internet-infamous, they can have severe reactions to the overwhelming tide of notes from strangers that they hate you personally. It pokes at the social parts of our brain, compelling behaviors and triggering rewards, and this is predictably bad for people, and particularly youth.</p>
<p>I cannot tell for sure what the specific risks are here, but I&rsquo;m confident they&rsquo;re there. We ran headlong into this with social media just a few years ago; I would much prefer if we could get ahead of it, and tread cautiously. This doesn&rsquo;t mean &ldquo;don&rsquo;t use LLMs&rdquo;. This means &ldquo;<strong>consider how you interact with them</strong>&rdquo;.</p>
<h3 id="ai-psychosis" class="relative group">AI Psychosis <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#ai-psychosis" aria-label="Anchor">#</a></span></h3><p>Given this section, we can&rsquo;t ignore the headlines. Some users of LLMs have developed psychosis, and it <a href="https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis" target="_blank" rel="noreferrer">appears to be connected to their use of LLMs</a>.</p>
<p>Mind that the media biases for sensationalism, and most experts will qualify this, but you do occasionally see claims like &ldquo;ChatGPT is driving people insane&rdquo; which is reductive to the point of being untrue. The article argues that it may reinforce and amplify delusions in users, and I believe that this is probably true. Nonetheless, this highlights that this problem is not merely theoretical, and we cannot afford to wait for in-depth scientific studies. It seems like LLMs present mental health risks, and users of LLMs should actively consider those risks in how they engage with LLMs.</p>
<h2 id="data" class="relative group">Data <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#data" aria-label="Anchor">#</a></span></h2><p>Because they present as so human, there&rsquo;s an argument to be made that they invite intimate information. Your search history can tell a lot about you, but there&rsquo;s a lot it leaves out. A conversation with a LLM on the same topic can go into much more detail, and be much better tailored to you. It presents as friendly and helpful, and can generally do a better job of answering your questions the more information you give it. This creates risks.</p>
<p>Imagine a cybersecurity incident occurs at one of the big LLM companies. Every conversation of every user, linked to their registered email address, and the stored memories about them, are all leaked to the internet. Anyone can now see everything entered into the chat. That&rsquo;s a privacy nightmare, the likes of which we&rsquo;ve never really seen before. It&rsquo;s a thing that <em>can happen</em>. And it&rsquo;s worth keeping in mind. If you&rsquo;re using a service for free, they&rsquo;re likely going to use some of your chats (likely redacted) to train their models, at least by default. <a href="https://openai.com/index/response-to-nyt-data-demands/" target="_blank" rel="noreferrer">OpenAI has also been legally required to hold onto all data, even deleted chats, pending some litigation</a>. This isn&rsquo;t their fault, but again, this is the kind of thing that <em>can happen</em>.</p>
<p>It is, as ever, good practice to carefully consider what data you are uploading and who you are uploading it to, and to understand the privacy policy and settings of any providers that you use. The fact that LLMs present to our brains as &ldquo;human-ish&rdquo; also increases the risk we absent-mindedly add more data than we mean to.</p>
<h1 id="hygiene" class="relative group">Hygiene <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#hygiene" aria-label="Anchor">#</a></span></h1><p>It is hard to predict exactly what the risks will be. The main thing that I am advocating for here is to be <em>cautious</em> and <em>mindful</em> in the ways we use LLMs, including of your own mental state, and that is the most important thing to do. There is still a lot that we don&rsquo;t know, but here are some things that I practice in order to make sure I&rsquo;m using LLMs carefully:</p>
<ul>
<li><strong>Have a clear goal</strong>: Having a clear goal in mind for use with the LLM helps to stay on the rails, manage context, and avoid getting sidetracked.</li>
<li><strong>Set the terms of the conversation</strong>: LLMs are built to align their outputs to user preferences. They will respond if you ask them to respond a different way. Instructing them on how to format their output, as well as your preferences for conversation, can go a long way and keep you in control of the conversation, and keeps your brain in &ldquo;tool use&rdquo; mode.
<ul>
<li>Setting the &ldquo;system prompt&rdquo; or other persistent instructions for your LLM is a good way to do this for general use.</li>
</ul>
</li>
<li><strong>Avoid emotional interaction</strong>: LLMs aren&rsquo;t sentient, they&rsquo;re not people, but to your brain, they present like they are. To my way of thinking, much ill can come from emotionally relying on them, or asking them for emotionally-tinged advice. This can probably be done in a careful way, but I think this is a risky dynamic.</li>
<li><strong>Control the facts</strong>: No fact that is output by an LLM can be trusted on its own. The best strategy I&rsquo;ve found is to identify and curate the facts presented to an LLM, prior giving it a task. Alterntively, if the LLM is linked to a search engine, then one might use that, but dig into the citations to make sure they&rsquo;re correct and being interpreted correctly. A strong chain of custody of &ldquo;the facts&rdquo; goes a long way to avoiding problems from hallucinations.
<ul>
<li>For fact-intensive tasks, consider using a search engine instead, or first. LLMs cannot add much to tasks like this, but they can make serious errors.</li>
</ul>
</li>
<li><strong>Avoid Oversharing</strong>: Consider carefully where you are uploading to, and what information you want them to have access to. It is often worth redacting personal infromation before sending messages to chatbots.</li>
<li><strong>Solicit Questions</strong>: A tactic I&rsquo;ve gotten a lot of value from is asking the LLM for a list of questions that will be helpful to know prior to starting the task. This gets the LLM to identify important factors ahead of time, identifying unknown-unknowns, so that we can provide factual, relevant inputs to the task from the start. It also helps stay on a good track by getting high-quality, factual information into the chat.</li>
<li><strong>Choosing to Ignore</strong>: LLMs produce a lot of output. Selectively choosing to ignore some of the output can improve your results. As a rule of thumb, I think it&rsquo;s a sign of good LLM use to go over the answer, choose bits to accept, choose bits to ignore, and move on, instead of taking the whole thing at face value.</li>
</ul>
<p>By considering what your goals are, and being mindful, you can get a lot more out of LLMs and avoid some of the pitfalls that we&rsquo;ve been seeing. This will be an important muscle to exercise as chatbots grow in prominence and presence. Just like it would&rsquo;ve been good to have &ldquo;social media hygiene&rdquo; in the early days of social media, it will be beneficial to exercise and think about &ldquo;LLM hygiene&rdquo; in the early days of LLMs. Take control of the way you use this technology, and be mindful of your processes.</p>

      </div>
    </section>
    <footer class="max-w-prose pt-8 print:hidden">
      
  <div class="flex">
    
    
    
      
      
    
    <div class="place-self-center">
      
        <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
          Author
        </div>
        <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
          Eric Miller
        </div>
      
      
        <div class="text-sm text-neutral-700 dark:text-neutral-400">Software Engineer, Kuberneter</div>
      
      <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          style="will-change:transform;"
          href="https://github.com/sosheskaz/"
          target="_blank"
          aria-label="Github"
          rel="me noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></a
        >
      
    
      
        <a
          class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
          style="will-change:transform;"
          href="https://www.linkedin.com/in/eric-miller/"
          target="_blank"
          aria-label="Linkedin"
          rel="me noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a
        >
      
    
  </div>

</div>
    </div>
  </div>


      
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:1313/posts/2025-09-06-llms-2/&amp;quote=Large%20Language%20Models,%20pt.%202:%20Safety%20and%20Hygiene"
          title="Share on Facebook"
          aria-label="Share on Facebook"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://x.com/intent/tweet/?url=http://localhost:1313/posts/2025-09-06-llms-2/&amp;text=Large%20Language%20Models,%20pt.%202:%20Safety%20and%20Hygiene"
          title="Post on X"
          aria-label="Post on X"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://tootpick.org/#text=http://localhost:1313/posts/2025-09-06-llms-2/%20Large%20Language%20Models,%20pt.%202:%20Safety%20and%20Hygiene"
          title="Toot on Mastodon"
          aria-label="Toot on Mastodon"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M433 179.11c0-97.2-63.71-125.7-63.71-125.7-62.52-28.7-228.56-28.4-290.48 0 0 0-63.72 28.5-63.72 125.7 0 115.7-6.6 259.4 105.63 289.1 40.51 10.7 75.32 13 103.33 11.4 50.81-2.8 79.32-18.1 79.32-18.1l-1.7-36.9s-36.31 11.4-77.12 10.1c-40.41-1.4-83-4.4-89.63-54a102.54 102.54 0 0 1-.9-13.9c85.63 20.9 158.65 9.1 178.75 6.7 56.12-6.7 105-41.3 111.23-72.9 9.8-49.8 9-121.5 9-121.5zm-75.12 125.2h-46.63v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.33V197c0-58.5-64-56.6-64-6.9v114.2H90.19c0-122.1-5.2-147.9 18.41-175 25.9-28.9 79.82-30.8 103.83 6.1l11.6 19.5 11.6-19.5c24.11-37.1 78.12-34.8 103.83-6.1 23.71 27.3 18.4 53 18.4 175z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:1313/posts/2025-09-06-llms-2/&amp;title=Large%20Language%20Models,%20pt.%202:%20Safety%20and%20Hygiene"
          title="Share on LinkedIn"
          aria-label="Share on LinkedIn"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a
        >
      
    
      
        <a
          class="m-1 inline-block min-w-[2.4rem] rounded bg-neutral-300 p-1 text-center text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="mailto:?body=http://localhost:1313/posts/2025-09-06-llms-2/&amp;subject=Large%20Language%20Models,%20pt.%202:%20Safety%20and%20Hygiene"
          title="Send via email"
          aria-label="Send via email"
          target="_blank"
          rel="noopener noreferrer"
          ><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>
</span></a
        >
      
    
  </section>


      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="group flex" href="http://localhost:1313/posts/2025-07-28-large-language-models/">
              <span
                class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&larr;</span
                ><span class="ltr:hidden rtl:inline">&rarr;</span></span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Large Language Models</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-07-28 00:00:00 -0500 CDT">July 28, 2025</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="group flex text-right" href="http://localhost:1313/posts/2025-09-07-llms-3/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Large Language Models, pt. 3: Context</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2025-09-04 15:09:11 -0500 CDT">September 4, 2025</time>
                  
                </span>
              </span>
              <span
                class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&rarr;</span
                ><span class="ltr:hidden rtl:inline">&larr;</span></span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

      </main>
      
        <div
          class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"
          id="to-top"
          hidden="true"
        >
          <a
            href="#the-top"
            class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
            aria-label="Scroll to top"
            title="Scroll to top"
          >
            &uarr;
          </a>
        </div>
      <footer class="py-10 print:hidden">
  
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2025
            Eric Miller
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://github.com/jpanther/congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    <div class="flex flex-row items-center">
      
      
      
      
        <div
          class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        >
          <button id="appearance-switcher-0" type="button" aria-label="appearance switcher">
            <div
              class="flex h-12 w-12 items-center justify-center dark:hidden"
              title="Switch to dark appearance"
            >
              <span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
            </div>
            <div
              class="hidden h-12 w-12 items-center justify-center dark:flex"
              title="Switch to light appearance"
            >
              <span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
            </div>
          </button>
        </div>
      
    </div>
  </div>
  
  
</footer>

    </div>
  </body>
</html>
